{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreejith-nair511/Summer_course_Ai/blob/main/LSTM_using_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WY1uNYFU9F_w",
        "outputId": "534e3e31-4b83-444a-f8b6-2e3f7d215eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.7500 - loss: 0.6920 - val_accuracy: 0.0000e+00 - val_loss: 0.6948\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8750 - loss: 0.6896 - val_accuracy: 0.5000 - val_loss: 0.6948\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.7500 - loss: 0.6870 - val_accuracy: 0.5000 - val_loss: 0.6948\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.7500 - loss: 0.6871 - val_accuracy: 0.5000 - val_loss: 0.6947\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7500 - loss: 0.6805 - val_accuracy: 0.5000 - val_loss: 0.6946\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.6821 - val_accuracy: 0.5000 - val_loss: 0.6944\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8750 - loss: 0.6752 - val_accuracy: 0.5000 - val_loss: 0.6941\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8750 - loss: 0.6748 - val_accuracy: 0.5000 - val_loss: 0.6938\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7500 - loss: 0.6717 - val_accuracy: 0.5000 - val_loss: 0.6933\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8750 - loss: 0.6632 - val_accuracy: 0.5000 - val_loss: 0.6928\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8750 - loss: 0.6588 - val_accuracy: 0.5000 - val_loss: 0.6919\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8750 - loss: 0.6554 - val_accuracy: 0.5000 - val_loss: 0.6909\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.6432 - val_accuracy: 0.5000 - val_loss: 0.6894\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8750 - loss: 0.6338 - val_accuracy: 0.5000 - val_loss: 0.6875\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8750 - loss: 0.6180 - val_accuracy: 0.5000 - val_loss: 0.6850\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8750 - loss: 0.6070 - val_accuracy: 0.5000 - val_loss: 0.6818\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8750 - loss: 0.5942 - val_accuracy: 0.5000 - val_loss: 0.6776\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8750 - loss: 0.5658 - val_accuracy: 0.5000 - val_loss: 0.6722\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8750 - loss: 0.5463 - val_accuracy: 0.5000 - val_loss: 0.6654\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8750 - loss: 0.5242 - val_accuracy: 0.5000 - val_loss: 0.6568\n",
            "Test Loss: 0.6568\n",
            "Test Accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Text Generation ---\n",
            "Seed: 'This is'\n",
            "Random Sampling (T=0.8): This is a great movie! i really enjoyed this the was the\n",
            "Nucleus Sampling (p=0.9): This is is is great i really enjoyed this enjoyed the movie\n",
            "Beam (Simplified Greedy): This is a great movie! i really enjoyed this film. film. film.\n",
            "\n",
            "--- Sentiment Classification of Generated Text ---\n",
            "Generated (Random): 'This is a great movie! i really enjoyed this the was the' -> Sentiment: Positive (Confidence: 0.55)\n",
            "Generated (Nucleus): 'This is is is great i really enjoyed this enjoyed the movie' -> Sentiment: Positive (Confidence: 0.54)\n",
            "Generated (Beam-like): 'This is a great movie! i really enjoyed this film. film. film.' -> Sentiment: Positive (Confidence: 0.52)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import random\n",
        "\n",
        "# 1. Prepare the dataset\n",
        "# For simplicity, let's create a dummy dataset.\n",
        "# In a real-world scenario, you would load your dataset (e.g., IMDB reviews).\n",
        "data = [\n",
        "    (\"This is a great movie!\", 1),\n",
        "    (\"I really enjoyed this film.\", 1),\n",
        "    (\"The acting was superb.\", 1),\n",
        "    (\"What a terrible experience.\", 0),\n",
        "    (\"I hated every minute of it.\", 0),\n",
        "    (\"This movie was awful.\", 0),\n",
        "    (\"It was okay, nothing special.\", 0),\n",
        "    (\"Definitely recommend it.\", 1),\n",
        "    (\"Waste of my time.\", 0),\n",
        "    (\"So happy I watched this.\", 1),\n",
        "]\n",
        "\n",
        "texts = [item[0] for item in data]\n",
        "labels = [item[1] for item in data]\n",
        "\n",
        "# Preprocessing\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "max_len = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, np.array(labels), test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Build the LSTM Model\n",
        "vocab_size = 5000\n",
        "embedding_dim = 100\n",
        "lstm_units = 150\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    LSTM(lstm_units),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=8, validation_data=(X_test, y_test))\n",
        "\n",
        "# 3. Sentiment Classification (Downstream Task)\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Now, let's demonstrate text generation *using sampling* which could hypothetically be\n",
        "# used for other tasks (e.g., generating synthetic text data, although not for sentiment classification directly\n",
        "# with this model unless the generation is sentiment-conditioned).\n",
        "\n",
        "# Get the trained model to generate text. For a classification model,\n",
        "# this isn't its primary purpose, but we can use its internal state (if it were designed for it)\n",
        "# or adapt it slightly for generation. A standard classification LSTM won't\n",
        "# have an output layer designed for generating the next token in a sequence.\n",
        "# A typical generative LSTM has a Dense layer with a softmax activation over the vocab size.\n",
        "\n",
        "# To demonstrate sampling methods for text generation, we need a model that outputs probabilities\n",
        "# over the vocabulary for the next token. The current model is a binary classifier.\n",
        "# Let's simulate a generative process using the trained model's internal representations or\n",
        "# if we were to adapt the model slightly for generation.\n",
        "# **NOTE:** This simulation is for demonstration purposes of sampling *methods*.\n",
        "# The generated text will not be meaningful for sentiment classification as the model\n",
        "# was trained for binary classification, not text generation.\n",
        "\n",
        "# To truly show text generation and sampling, we'd ideally train a separate language model.\n",
        "# However, sticking to the prompt's structure (using the *trained model* for a downstream task),\n",
        "# let's assume we want to generate text *related* to the input data (though again, the classifier output isn't ideal for this).\n",
        "\n",
        "# Let's create a simple function that *simulates* generating the next token's probability distribution\n",
        "# based on the final state of the LSTM when fed a sequence. This is a hacky way to demonstrate sampling\n",
        "# and not how a real generative model works.\n",
        "\n",
        "# A proper text generation setup would have:\n",
        "# 1. Train a language model (LSTM with softmax output over vocab)\n",
        "# 2. Implement sampling strategies to pick the next token based on the probability distribution.\n",
        "\n",
        "# Let's build a simple generative layer on top for demonstration.\n",
        "# This requires changing the model architecture slightly, which deviates from using the *trained* classifier directly.\n",
        "# The prompt is slightly ambiguous here. Let's interpret it as: train an LSTM (could be a language model),\n",
        "# use sampling methods for generation, and *then* use the *generated text* for sentiment classification.\n",
        "# This requires a separate sentiment classifier.\n",
        "\n",
        "# Let's train a simple language model first, then use sampling, then classify the generated text.\n",
        "\n",
        "# --- Revised Approach: Train Language Model -> Generate Text with Sampling -> Classify Generated Text ---\n",
        "\n",
        "# We need more text data to train a decent language model.\n",
        "# Let's use the same small dataset but treat it as a sequence for language modeling.\n",
        "# This will result in a very poor language model, but it serves to demonstrate the process.\n",
        "\n",
        "all_text = \" \".join(texts)\n",
        "sequences = []\n",
        "# Create sequences of tokens\n",
        "for i in range(1, len(all_text.split())):\n",
        "    sequence = all_text.split()[:i+1]\n",
        "    sequences.append(sequence)\n",
        "\n",
        "# Tokenize\n",
        "tokenizer_lm = Tokenizer()\n",
        "tokenizer_lm.fit_on_texts(sequences)\n",
        "sequences_lm = tokenizer_lm.texts_to_sequences(sequences)\n",
        "\n",
        "# Prepare input and output for language model\n",
        "X_lm, y_lm = [], []\n",
        "for seq in sequences_lm:\n",
        "    X_lm.append(seq[:-1])\n",
        "    y_lm.append(seq[-1])\n",
        "\n",
        "X_lm = pad_sequences(X_lm, maxlen=max_len, padding='pre') # Pad at the beginning for LM\n",
        "y_lm = tf.keras.utils.to_categorical(y_lm, num_classes=len(tokenizer_lm.word_index) + 1)\n",
        "\n",
        "# Build Language Model\n",
        "vocab_size_lm = len(tokenizer_lm.word_index) + 1\n",
        "model_lm = Sequential([\n",
        "    Embedding(vocab_size_lm, embedding_dim, input_length=max_len),\n",
        "    LSTM(lstm_units, return_sequences=False), # We predict the next word\n",
        "    Dense(vocab_size_lm, activation='softmax')\n",
        "])\n",
        "\n",
        "model_lm.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model_lm.summary()\n",
        "\n",
        "# Train Language Model\n",
        "# With this small dataset, training will be quick but the model will be weak.\n",
        "model_lm.fit(X_lm, y_lm, epochs=50, verbose=0) # Train for more epochs on tiny data\n",
        "\n",
        "# 4. Text Generation with Sampling Methods\n",
        "def generate_text(model, tokenizer, seed_text, max_sequence_length, num_generate, sampling_method, temperature=1.0, top_p=0.9):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(num_generate):\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "        # Pad the sequence to the expected input length\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "        # Get the next word probability distribution\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature\n",
        "        predicted_probs = np.log(predicted_probs + 1e-10) / temperature\n",
        "        exp_preds = np.exp(predicted_probs)\n",
        "        predicted_probs = exp_preds / np.sum(exp_preds) # Softmax after temperature\n",
        "\n",
        "        next_token_index = -1\n",
        "\n",
        "        if sampling_method == 'random':\n",
        "            next_token_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "        elif sampling_method == 'nucleus':\n",
        "            # Sort probabilities in descending order\n",
        "            sorted_preds = np.sort(predicted_probs)[::-1]\n",
        "            sorted_indices = np.argsort(predicted_probs)[::-1]\n",
        "\n",
        "            # Calculate cumulative probabilities\n",
        "            cumulative_probs = np.cumsum(sorted_preds)\n",
        "\n",
        "            # Find the smallest set of tokens whose cumulative probability exceeds top_p\n",
        "            nucleus = sorted_indices[cumulative_probs < top_p]\n",
        "            # Add the next token to ensure nucleus is not empty (edge case)\n",
        "            if len(nucleus) == 0:\n",
        "                nucleus = sorted_indices[:1] # Take the most probable token if nucleus is empty\n",
        "            elif cumulative_probs[len(nucleus)-1] < top_p: # Add the last token if needed to cross top_p\n",
        "                 nucleus = sorted_indices[:len(nucleus)+1]\n",
        "\n",
        "\n",
        "            # Filter probabilities to the nucleus\n",
        "            nucleus_probs = predicted_probs[nucleus]\n",
        "            nucleus_probs = nucleus_probs / np.sum(nucleus_probs) # Re-normalize\n",
        "\n",
        "            # Sample from the nucleus\n",
        "            next_token_index = np.random.choice(nucleus, p=nucleus_probs)\n",
        "\n",
        "        elif sampling_method == 'beam':\n",
        "             # Beam search is more complex and usually generates multiple sequences in parallel,\n",
        "             # keeping the top 'k' most probable sequences at each step.\n",
        "             # Implementing full beam search here would make the function much more complex.\n",
        "             # A simplified \"beam-like\" approach might just pick the top-k, but that's not true beam search.\n",
        "             # Let's stick to random and nucleus for a single-token generation demonstration.\n",
        "             # If you need full beam search, it requires tracking multiple candidate sequences.\n",
        "             # For a single token generation, it reduces to just picking the top 1 if beam width is 1,\n",
        "             # or considering top k if width is k (but still picking one based on some criteria).\n",
        "             # Let's simulate by just picking the single most probable word for simplicity\n",
        "             # to represent a deterministic \"beam\" of width 1. This isn't true beam search.\n",
        "             next_token_index = np.argmax(predicted_probs)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid sampling method\")\n",
        "\n",
        "        # Find the word from the index\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == next_token_index:\n",
        "                next_word = word\n",
        "                break\n",
        "        generated_text += \" \" + next_word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example usage of sampling methods\n",
        "seed_text = \"This is\"\n",
        "num_generate_words = 10\n",
        "max_seq_len_lm = max_len # Use the same max length for padding\n",
        "\n",
        "print(\"--- Text Generation ---\")\n",
        "print(f\"Seed: '{seed_text}'\")\n",
        "\n",
        "# Random Sampling\n",
        "generated_random = generate_text(model_lm, tokenizer_lm, seed_text, max_seq_len_lm, num_generate_words, 'random', temperature=0.8)\n",
        "print(f\"Random Sampling (T=0.8): {generated_random}\")\n",
        "\n",
        "# Nucleus Sampling (Top-p)\n",
        "generated_nucleus = generate_text(model_lm, tokenizer_lm, seed_text, max_seq_len_lm, num_generate_words, 'nucleus', top_p=0.9)\n",
        "print(f\"Nucleus Sampling (p=0.9): {generated_nucleus}\")\n",
        "\n",
        "# Beam \"Sampling\" (simplified - essentially greedy for a single token)\n",
        "# Note: This is NOT proper beam search.\n",
        "generated_beam = generate_text(model_lm, tokenizer_lm, seed_text, max_seq_len_lm, num_generate_words, 'beam')\n",
        "print(f\"Beam (Simplified Greedy): {generated_beam}\")\n",
        "\n",
        "\n",
        "# 5. Sentiment Classification of Generated Text\n",
        "# To classify the generated text, we need to use the *original* sentiment classification model.\n",
        "# This assumes the generated text is relevant to the sentiment task.\n",
        "# The generated text from our simple LM trained on sentence fragments won't be meaningful for sentiment.\n",
        "# However, to fulfill the prompt, we will classify the generated strings as if they were reviews.\n",
        "\n",
        "# Re-use the original tokenizer and classification model\n",
        "tokenizer_clf = Tokenizer(num_words=5000) # Recreate/load the original tokenizer\n",
        "tokenizer_clf.fit_on_texts(texts) # Fit on the original training data\n",
        "\n",
        "def classify_sentiment(text, model, tokenizer, max_sequence_length):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
        "    prediction = model.predict(padded_sequence, verbose=0)[0][0]\n",
        "    sentiment = 'Positive' if prediction > 0.4 else 'Negative'\n",
        "    confidence = prediction if sentiment == 'Positive' else 1 - prediction\n",
        "    return sentiment, confidence\n",
        "\n",
        "print(\"\\n--- Sentiment Classification of Generated Text ---\")\n",
        "\n",
        "sentiment_random, confidence_random = classify_sentiment(generated_random, model, tokenizer_clf, max_len)\n",
        "print(f\"Generated (Random): '{generated_random}' -> Sentiment: {sentiment_random} (Confidence: {confidence_random:.2f})\")\n",
        "\n",
        "sentiment_nucleus, confidence_nucleus = classify_sentiment(generated_nucleus, model, tokenizer_clf, max_len)\n",
        "print(f\"Generated (Nucleus): '{generated_nucleus}' -> Sentiment: {sentiment_nucleus} (Confidence: {confidence_nucleus:.2f})\")\n",
        "\n",
        "sentiment_beam, confidence_beam = classify_sentiment(generated_beam, model, tokenizer_clf, max_len)\n",
        "print(f\"Generated (Beam-like): '{generated_beam}' -> Sentiment: {sentiment_beam} (Confidence: {confidence_beam:.2f})\")\n",
        "\n",
        "# Note: The sentiment classification results for the generated text will likely be random or consistently biased\n",
        "# because the generated text itself is not coherent or sentiment-rich due to the tiny training data and simple LM.\n",
        "# This part demonstrates the *process* of using a separate model to classify generated text, not that the results are meaningful here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wC8LXZfCA283"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}