<h1>FLASTM CODE </h1>

Text Generation using LSTM (Brown Corpus)"
ğŸ”¹ 1. Package Installation & Import

!pip install nltk tensorflow pandas numpy
Installs required libraries. Then imports:

nltk: for downloading and processing the Brown corpus

tensorflow.keras: for building the LSTM

pandas, numpy: for data handling

ğŸ”¹ 2. Data Preparation from Brown Corpus


nltk.download('brown')
nltk.download('punkt')

from nltk.corpus import brown
from nltk.tokenize import word_tokenize

paragraphs = brown.paras(categories='news')
Downloads NLTK datasets.

Selects news category paragraphs from Brown Corpus.

Tokenizes and flattens into sentences â†’ words.

Then:


words = word_tokenize(text.lower())
Lowercases all text and tokenizes into words.

ğŸ”¹ 3. Tokenizer & Sequence Creation

tokenizer = Tokenizer(oov_token="<OOV>")
tokenizer.fit_on_texts([words])
Builds vocabulary from words.

Handles out-of-vocabulary tokens using <OOV>.


# Create sequences of words
for i in range(1, len(words)):
    sequence = words[max(0, i-20):i]
    input_sequences.append(sequence)
Creates sliding window input sequences of 20 words each.

ğŸ”¹ 4. Convert to Integer Sequences + Padding

sequences = tokenizer.texts_to_sequences(input_sequences)
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')
Converts each word to its integer index and pads sequences to a uniform length.

ğŸ”¹ 5. Split Inputs and Outputs

xs = padded_sequences[:, :-1]  # input
ys = padded_sequences[:, -1]   # output
ys = to_categorical(ys, num_classes=total_words)
Splits data into xs (predictors) and ys (next word).

Converts ys to one-hot vectors.

ğŸ”¹ 6. Build & Train LSTM Model

model = Sequential()
model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))
model.add(LSTM(150))
model.add(Dense(total_words, activation='softmax'))
Embedding Layer: Maps tokens to dense vectors.

LSTM Layer: Learns sequence patterns.

Dense Softmax Layer: Predicts next word among vocabulary.


model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(xs, ys, epochs=20)
Trains the model for 20 epochs.

ğŸ”¹ 7. Save Model & Tokenizer

model.save_weights('brown_lstm.weights.h5')
with open('tokenizer.pkl', 'wb') as handle:
    pickle.dump(tokenizer, handle)
with open('max_sequence_len.pkl', 'wb') as handle:
    pickle.dump(max_sequence_len, handle)
Saves model weights.

Saves tokenizer and maximum sequence length for future use.

âœ… What You Can Do With This:
Text generation: After training, you can give a seed sentence and let the model generate the next word(s).

Fine-tuning: Replace Brown corpus with custom datasets.

Deployment: Wrap into a Flask app or Colab demo.










You said:
what does picke means and oinkt


ChatGPT said:
Great question, bro! Letâ€™s break it down in a super beginner-friendly way â€” like how you'd explain it to your younger sibling who just started coding. ğŸ§ƒ

ğŸ¥’ What is pickle in Python?
pickle is a Python module used to save Python objects (like models, data structures, tokenizers, etc.) into a file â€” so that you can load them back later without re-creating them from scratch.

Think of pickle like zipping your data into a .pkl file, so you can:

ğŸ§  Save your trained model or tokenizer

ğŸ” Reuse them later without retraining

ğŸ“¦ Transfer them to someone else easily

ğŸ Example:
Letâ€™s say you have a trained tokenizer object:

python
Copy
Edit
tokenizer = Tokenizer()
To save it:

python
Copy
Edit
import pickle

with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)
To load it later:

python
Copy
Edit
with open('tokenizer.pkl', 'rb') as f:
    tokenizer = pickle.load(f)
ğŸ“Œ What does .pkl mean?
.pkl is just the file extension used for files saved with pickle.

Just like:

.txt â†’ text file

.csv â†’ comma-separated values

.pkl â†’ pickled Python object

ğŸ· Why is it called pickle?
Because you're preserving an object for later, like how we preserve cucumbers as pickles. Kinda cute, right? ğŸ¥’ğŸ˜„

